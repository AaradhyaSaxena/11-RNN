{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content based book recommendation system**, which will determine which books are close to each other based on how similar the discussed topics are. The dataset consists of books written by Darwin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "import re, os\n",
    "import pickle\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import similarities\n",
    "from gensim import corpora\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets_books\\\\Autobiography.txt',\n",
       " 'datasets_books\\\\CoralReefs.txt',\n",
       " 'datasets_books\\\\DescentofMan.txt',\n",
       " 'datasets_books\\\\DifferentFormsofFlowers.txt',\n",
       " 'datasets_books\\\\EffectsCrossSelfFertilization.txt',\n",
       " 'datasets_books\\\\ExpressionofEmotionManAnimals.txt',\n",
       " 'datasets_books\\\\FormationVegetableMould.txt',\n",
       " 'datasets_books\\\\FoundationsOriginofSpecies.txt',\n",
       " 'datasets_books\\\\GeologicalObservationsSouthAmerica.txt',\n",
       " 'datasets_books\\\\InsectivorousPlants.txt',\n",
       " 'datasets_books\\\\LifeandLettersVol1.txt',\n",
       " 'datasets_books\\\\LifeandLettersVol2.txt',\n",
       " 'datasets_books\\\\MonographCirripedia.txt',\n",
       " 'datasets_books\\\\MonographCirripediaVol2.txt',\n",
       " 'datasets_books\\\\MovementClimbingPlants.txt',\n",
       " 'datasets_books\\\\OriginofSpecies.txt',\n",
       " 'datasets_books\\\\PowerMovementPlants.txt',\n",
       " 'datasets_books\\\\VariationPlantsAnimalsDomestication.txt',\n",
       " 'datasets_books\\\\VolcanicIslands.txt',\n",
       " 'datasets_books\\\\VoyageBeagle.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"datasets_books/\"\n",
    "files = glob.glob(folder + '*.txt')\n",
    "files.sort()\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123231,\n",
       " 496068,\n",
       " 1776539,\n",
       " 617088,\n",
       " 913713,\n",
       " 624232,\n",
       " 335920,\n",
       " 523021,\n",
       " 797401,\n",
       " 901406,\n",
       " 1047518,\n",
       " 1010643,\n",
       " 767492,\n",
       " 1660866,\n",
       " 298319,\n",
       " 916267,\n",
       " 1093567,\n",
       " 1043499,\n",
       " 341447,\n",
       " 1149574]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts = []\n",
    "titles = []\n",
    "\n",
    "for n in files:\n",
    "    f = open(n, encoding='utf-8-sig')\n",
    "    # Remove non-alpha-numeric characters\n",
    "    data = re.sub('[\\W_]+', ' ', f.read())\n",
    "    # Store the texts and titles of the books\n",
    "    titles.append(os.path.basename(n).replace('.txt', ''))\n",
    "    txts.append(data)\n",
    "\n",
    "[len(t) for t in txts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Finding Darwin's most famous book: \"On the Origin of Species\" for analysis\n",
    "for i in range(len(titles)):\n",
    "    if(titles[i]==\"OriginofSpecies\"):\n",
    "        ori = i\n",
    "print(ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the corpus. <br/>\n",
    "Transforming each text into a list of the individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on',\n",
       " 'origin',\n",
       " 'species',\n",
       " 'but',\n",
       " 'with',\n",
       " 'regard',\n",
       " 'material',\n",
       " 'world',\n",
       " 'can',\n",
       " 'least']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a list of stop words\n",
    "stoplist = set('for a of the and to in to be which some is at that we i who whom show via may my our might as well'.split())\n",
    "\n",
    "txts_lower_case = [txt.lower() for txt in txts]\n",
    "txts_split = [txt.split() for txt in txts_lower_case]\n",
    "# Removing tokens which are part of the list of stop words\n",
    "texts = [[word for word in txt if word not in stoplist] for txt in txts_split]\n",
    "\n",
    "texts[ori][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_stem = pickle.load(open(\"datasets_books/texts_stem.p\", \"rb\"))\n",
    "\n",
    "texts_stem[ori][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts_stem)\n",
    "bows = [dictionary.doc2bow(txt) for txt in texts_stem]\n",
    "\n",
    "bows[ori][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most common words of a given book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow_origin = pd.DataFrame(bows[ori])\n",
    "\n",
    "df_bow_origin.columns = ['index', 'occurrences']\n",
    "df_bow_origin['token'] = df_bow_origin['index'].apply(lambda x: dictionary[x])\n",
    "\n",
    "df_bow_origin = df_bow_origin.sort_values('occurrences', ascending=False)\n",
    "df_bow_origin.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a tf-idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TfidfModel(bows)\n",
    "\n",
    "model[bows[ori]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(model[bows[ori]])\n",
    "\n",
    "df_tfidf.columns = ['id', 'score']\n",
    "df_tfidf['token'] = df_tfidf['id'].apply(lambda x: dictionary[x])\n",
    "\n",
    "df_tfidf = df_tfidf.sort_values('score', ascending=False)\n",
    "df_tfidf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute distance between texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = similarities.MatrixSimilarity(model[bows])\n",
    "\n",
    "sim_df = pd.DataFrame(list(sims))\n",
    "sim_df.columns = titles\n",
    "sim_df.index = titles\n",
    "\n",
    "sim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book most similar to \"On the Origin of Species\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = sim_df['OriginofSpecies']\n",
    "\n",
    "v_sorted = v.sort_values()\n",
    "v_sorted.plot.barh(x='lab', y='val', rot=0).plot()\n",
    "\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Book\")\n",
    "plt.title(\"Similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which books have similar content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the clusters from the similarity matrix,\n",
    "# using the Ward variance minimization algorithm\n",
    "Z = hierarchy.linkage(sims, 'ward')\n",
    "\n",
    "hierarchy.dendrogram(Z, leaf_font_size=8, labels=sim_df.index, orientation='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
